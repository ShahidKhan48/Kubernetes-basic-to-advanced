mimir-distributed:
  admin-cache:
    enabled: true
    replicas: 2
  admin_api:
    replicas: 1
  alertmanager:
    persistentVolume:
      enabled: true
    replicas: 2
    resources:
      limits:
        memory: 1.4Gi
      requests:
        memory: 1Gi
    statefulSet:
      enabled: true

  chunks-cache:
    allocatedMemory: 500
    enabled: true
    maxItemMemory: 1
    replicas: 2
  compactor:
    persistentVolume:
      size: 10Gi
    nodeSelector: 
      workloadType: "amd-spot"
    tolerations: 
      - key: "scheduling.cast.ai/node-template"
        value: "amd-spot"
        operator: "Equal"
        effect: "NoSchedule"
    resources:
      limits:
        memory: 5Gi
      requests:
        memory: 5Gi

  continuous_test:
    auth:
      password: c6Sd3W7pgIsBc08!
      tenant: observe
      type: basicAuth
    enabled: false
    maxQueryAge: 48h
    numSeries: 100000
    runInterval: 10s
  distributor:
    extraArgs:
      distributor.ingestion-rate-limit: "10000000000000"
    replicas: 3
    resources:
      limits:
        memory: 5.7Gi
      requests:
        memory: 4Gi

  gateway:
    replicas: 1
  image:
    repository: grafana/mimir
    tag: 2.12.0
  index-cache:
    enabled: true
    replicas: 3
  ingester:

    affinity: {}
    nodeSelector: 
      workloadType: "amd-spot"
    tolerations: 
      - key: "scheduling.cast.ai/node-template"
        value: "amd-spot"
        operator: "Equal"
        effect: "NoSchedule"
    extraArgs:
      ingester.max-global-series-per-metric: "0"
      ingester.max-global-series-per-user: "0"
    persistentVolume:
      size: 20Gi
    replicas: 3
    resources:
      limits:
        memory: 8Gi
      requests:
        memory: 5Gi
    zoneAwareReplication:
      enabled: false

  memcached:
    image:
      repository: memcached
      tag: 1.6.19-alpine
  memcachedExporter:
    enabled: true
  metaMonitoring:
    serviceMonitor:
      clusterLabel: null
      enabled: true
      labels:
        release: monitor
  metadata-cache:
    enabled: true
  mimir:
    config: |
      usage_stats:
        installation_mode: helm
      multitenancy_enabled: true
      activity_tracker:
        filepath: /active-query-tracker/activity.log


      alertmanager:
        data_dir: /data
        enable_api: true
        external_url: /alertmanager
        fallback_config_file: /configs/alertmanager_fallback_config.yaml

      alertmanager_storage:
        backend: s3
        s3:
          bucket_name: ninjacart-mimir-rule-jbmterem7tyfzo9medhus65s56y5caps3b-s3alias


      blocks_storage:
        backend: s3
        s3:
          bucket_name: ninjacart-mimir-7ywhbstj1bdthzm8ya7p5igik633gaps3a-s3alias
        bucket_store:
          # max_chunk_pool_bytes: 12884901888 # 12GiB
          {{- if index .Values "chunks-cache" "enabled" }}
          chunks_cache:
            backend: memcached
            memcached:
              addresses: {{ include "mimir.chunksCacheAddress" . }}
              max_item_size: {{ mul (index .Values "chunks-cache").maxItemMemory 1024 1024 }}
              timeout: 450ms
              max_idle_connections: 150
          {{- end }}
          {{- if index .Values "index-cache" "enabled" }}
          index_cache:
            backend: memcached
            memcached:
              addresses: {{ include "mimir.indexCacheAddress" . }}
              max_item_size: {{ mul (index .Values "index-cache").maxItemMemory 1024 1024 }}
              max_idle_connections: 150
          {{- end }}
          {{- if index .Values "metadata-cache" "enabled" }}
          metadata_cache:
            backend: memcached
            memcached:
              addresses: {{ include "mimir.metadataCacheAddress" . }}
              max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
              max_idle_connections: 150
          {{- end }}
          sync_dir: /data/tsdb-sync
        tsdb:
          dir: /data/tsdb

      compactor:
        compaction_interval: 30m
        deletion_delay: 2h
        max_closing_blocks_concurrency: 2
        max_opening_blocks_concurrency: 4
        symbols_flushers_concurrency: 4
        data_dir: "/data"
        sharding_ring:
          wait_stability_min_duration: 1m

      frontend:
        log_queries_longer_than: 1m
        parallelize_shardable_queries: true
        {{- if index .Values "results-cache" "enabled" }}
        results_cache:
          backend: memcached
          memcached:
            timeout: 500ms
            addresses: {{ include "mimir.resultsCacheAddress" . }}
            max_item_size: {{ mul (index .Values "results-cache").maxItemMemory 1024 1024 }}
        cache_results: true
        {{- end }}
        {{- if .Values.query_scheduler.enabled }}
        scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
        {{- end }}

      frontend_worker:
        grpc_client_config:
          max_send_msg_size: 419430400 # 400MiB
        {{- if .Values.query_scheduler.enabled }}
        scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
        {{- else }}
        frontend_address: {{ template "mimir.fullname" . }}-query-frontend-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
        {{- end }}

      ingester:
        ring:
          final_sleep: 0s
          num_tokens: 512
          tokens_file_path: /data/tokens
          unregister_on_shutdown: false
          zone_awareness_enabled: false
          replication_factor: 1

      ingester_client:
        grpc_client_config:
          max_recv_msg_size: 104857600
          max_send_msg_size: 104857600


      limits:
        max_label_names_per_series: 120
        max_global_series_per_user: 12000000 # To accomodate one big tenant
        ingestion_rate: 400000
        ingestion_burst_size: 8000000
        out_of_order_time_window: 1h
        ruler_max_rules_per_rule_group: 100
        ruler_max_rule_groups_per_tenant: 500
        compactor_blocks_retention_period: 90d
        accept_ha_samples: true
        ha_cluster_label: cluster
        
      memberlist:
        abort_if_cluster_join_fails: false
        compression_enabled: false
        cluster_label: "mimir-distributed-gossip-ring.observe.svc.cluster.local"
        cluster_label_verification_disabled: true
        bind_addr:
        - ${MY_POD_IP}
        join_members:
        - dns+mimir-distributed-gossip-ring:7946

      querier:
        # With query sharding we run more but smaller queries. We must strike a balance
        # which allows us to process more sharded queries in parallel when requested, but not overload
        # queriers during non-sharded queries.
        max_concurrent: 16

      query_scheduler:
        # Increase from default of 100 to account for queries created by query sharding
        max_outstanding_requests_per_tenant: 800

      ruler:
        alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.{{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/alertmanager
        enable_api: true
        rule_path: /data

      ruler_storage:
        backend: s3
        s3:
          bucket_name: ninjacart-mimir-rule-jbmterem7tyfzo9medhus65s56y5caps3b-s3alias
      runtime_config:
        file: /var/{{ include "mimir.name" . }}/runtime.yaml
      distributor:
        instance_limits:
          max_ingestion_rate: 60000 # Per-distributor rate limit
          max_inflight_push_requests: 2500
        ha_tracker:
          enable_ha_tracker: true
          kvstore:
            store: etcd
            etcd:
              endpoints: ["etcd:2379"]

      server:
        grpc_server_max_concurrent_streams: 1000


      store_gateway:
        sharding_ring:
          wait_stability_min_duration: 1m
          zone_awareness_enabled: false
          replication_factor: 1
  minio:
    enabled: false
  nginx:
    basicAuth:
      enabled: true
      password: c6Sd3W7pgIsBc08!
      username: observe
    replicas: 3
  overrides_exporter:
    replicas: 1
    resources: null

  querier:
    extraArgs:
      querier.max-fetched-chunks-per-query: "8000000"
    replicas: 2
    resources:
      limits:
        memory: 5.6Gi
      requests:
        memory: 4Gi

  query_frontend:
    replicas: 2
    resources:
      limits:
        memory: 2.8Gi
      requests:
        memory: 2Gi
  rbac:
    type: null
    usePodSecurityPolicy: false
  results-cache:
    enabled: true
    replicas: 2
  rollout_operator:
    enabled: false
  ruler:
    replicas: 2
    resources:
      limits:
        memory: 2.8Gi
      requests:
        memory: 2Gi

  runtimeConfig:
    ingester_limits:
      max_inflight_push_requests: 2500
      max_ingestion_rate: 300000
      max_series: 10000000
  store_gateway:

    affinity: {}
    nodeSelector: 
      workloadType: "amd-spot"
    tolerations: 
      - key: "scheduling.cast.ai/node-template"
        value: "amd-spot"
        operator: "Equal"
        effect: "NoSchedule"
    persistentVolume:
      size: 10Gi
    replicas: 3
    resources:
      limits:
        memory: 2.1Gi
      requests:
        memory: 1.5Gi

    zoneAwareReplication:
      enabled: false
  serviceAccount:
    annotations: 
      eks.amazonaws.com/role-arn: arn:aws:iam::061039808372:role/mimir-distributed-role
    automountServiceAccountToken: true
    create: true
    imagePullSecrets: []
    labels: {}
    name: null

  global:
    extraArgs:
    - -config.expand-env=true
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  



# grpc_server_max_connection_age: 2m
# grpc_server_max_connection_age_grace: 2562047h47m16.854775807s
# grpc_server_max_connection_idle: 2562047h47m16.854775807s